{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'PlottingHelpers' from '/home/jovyan/python-ml-turbofan/PlottingHelpers.py'>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import importlib\n",
    "\n",
    "#import tensorflow\n",
    "\n",
    "from LoadData import load_data\n",
    "import PlottingHelpers\n",
    "import ProcessingHelpers\n",
    "\n",
    "importlib.reload(ProcessingHelpers) # while still working on than fun\n",
    "importlib.reload(PlottingHelpers) # while still working on than fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "dirname = os.getcwd()\n",
    "pth = os.path.join(dirname, 'CMAPSSData')\n",
    "\n",
    "print('loading data...')\n",
    "dc = load_data(pth)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the first data set training data\n",
    "df = dc['FD_001']['df_train'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a Column for the RUL target data (y)\n",
    "\n",
    "According to the data description document the data set contains multiple units, each unit starts at a certain degradation point and the measurement data ends closely before the unit was decommissioned of broke. \n",
    "\n",
    "Therefore assume, that for the last measurement time that is available for a unit the units RUL=0 (stopped measuring just before machine broke)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the time of the last available measurement for each unit\n",
    "mapper = {}\n",
    "for unit_nr in df['unit_nr'].unique():\n",
    "    mapper[unit_nr] = df['time'].loc[df['unit_nr'] == unit_nr].max()\n",
    "    \n",
    "# calculate RUL = time.max() - time_now for each unit\n",
    "df['RUL'] = df['unit_nr'].apply(lambda nr: mapper[nr]) - df['time']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop the nan columns and rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with all nan: \n",
      "['sensor_22', 'sensor_23', 'sensor_24', 'sensor_25', 'sensor_26']\n",
      "\n",
      "Columns with all const values: \n",
      "['os_3', 'sensor_01', 'sensor_05', 'sensor_06', 'sensor_10', 'sensor_16', 'sensor_18', 'sensor_19', 'sensor_22', 'sensor_23', 'sensor_24', 'sensor_25', 'sensor_26']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cols_nan = df.columns[df.isna().any()].tolist()\n",
    "print('Columns with all nan: \\n' + str(cols_nan) + '\\n')\n",
    "\n",
    "cols_const = [ col for col in df.columns if len(df[col].unique()) <= 2 ]\n",
    "print('Columns with all const values: \\n' + str(cols_const) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=cols_const + cols_nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "take out a certain percentage of units from the training data set for testing later, (additionally to the classic validation methods)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "units = df['unit_nr'].unique()\n",
    "n_units = len(df['unit_nr'].unique())\n",
    "\n",
    "units_test = random.sample(list(units), int(n_units * 0.2))\n",
    "units_train = [nr for nr in units if nr not in units_test]\n",
    "\n",
    "test_data = df.loc[df['unit_nr'].apply( lambda x: x in units_test )].copy()\n",
    "train_data = df.loc[df['unit_nr'].apply( lambda x: x in units_train )].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize the dataset by mean and std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unit_nr     -1.666058\n",
      "time        -1.586195\n",
      "os_1        -0.315366\n",
      "os_2        -1.372469\n",
      "sensor_02   -1.703752\n",
      "sensor_03   -0.123536\n",
      "sensor_04   -0.911245\n",
      "sensor_07    1.105980\n",
      "sensor_08   -0.492709\n",
      "sensor_09   -0.878094\n",
      "sensor_11   -0.250016\n",
      "sensor_12    0.318936\n",
      "sensor_13   -1.031179\n",
      "sensor_14   -0.285608\n",
      "sensor_15   -0.595031\n",
      "sensor_17   -0.769869\n",
      "sensor_20    1.332677\n",
      "sensor_21    1.184062\n",
      "RUL          1.284536\n",
      "Name: 0, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Test data is *not* used when calculating the mean and std\n",
    "\n",
    "mean = train_data.mean(axis=0)\n",
    "std = train_data.std(axis=0)\n",
    "df_n_train = (train_data - mean) / std\n",
    "df_n_test = (test_data - mean) / std\n",
    "\n",
    "print(df_n_train.iloc[0])  # First training sample, normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "  model = keras.Sequential([\n",
    "    keras.layers.Dense(64, activation=tf.nn.relu,\n",
    "                       input_shape=(train_data.shape[1],)),\n",
    "    keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(1)\n",
    "  ])\n",
    "\n",
    "  optimizer = tf.train.RMSPropOptimizer(0.001)\n",
    "\n",
    "  model.compile(loss='mse',\n",
    "                optimizer=optimizer,\n",
    "                metrics=['mae'])\n",
    "  return model\n",
    "\n",
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display training progress by printing a single dot for each completed epoch\n",
    "class PrintDot(keras.callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs):\n",
    "    if epoch % 100 == 0: print('')\n",
    "    print('.', end='')\n",
    "\n",
    "EPOCHS = 500\n",
    "\n",
    "# Store training stats\n",
    "history = model.fit(train_data, train_labels, epochs=EPOCHS,\n",
    "                    validation_split=0.2, verbose=0,\n",
    "                    callbacks=[PrintDot()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "  plt.figure()\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Mean Abs Error')\n",
    "  plt.plot(history.epoch, np.array(history.history['mean_absolute_error']), label='Train Loss')\n",
    "  plt.plot(history.epoch, np.array(history.history['val_mean_absolute_error']), label = 'Val loss')\n",
    "  plt.legend()\n",
    "\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "\n",
    "# The patience parameter is the amount of epochs to check for improvement\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)\n",
    "\n",
    "history = model.fit(train_data, train_labels, epochs=EPOCHS,\n",
    "                    validation_split=0.2, verbose=0,\n",
    "                    callbacks=[early_stop, PrintDot()])\n",
    "\n",
    "plot_history(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[loss, mae] = model.evaluate(test_data, test_labels, verbose=0)\n",
    "print(\"Testing set Mean Abs Error: {:7.2f}\".format(mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(test_predictions - test_labels)\n",
    "plt.xlabel(\"Prediction Error\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
